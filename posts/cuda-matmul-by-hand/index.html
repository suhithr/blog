<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>How a tiled matmul works | Suhith</title><meta name=keywords content><meta name=description content="Writing a tiled CUDA matmul üöÄ
This year during my time at the Recurse Center, I worked through the various optimizations presented in Simon Boehm&rsquo;s iconic post How to Optimize a CUDA Matmul Kernel for cuBLAS-like Performance: a Worklog. I approached these as a series of puzzles where I would read as little as possible, just the title or the first paragraphs describing the algorithm and then implement it in CUDA/C++. This was an exercise in writing & debugging CUDA along with implementing kernel code from a high level algorithm."><meta name=author content="Suhith Rajesh"><link rel=canonical href=https://blog.suhith.com/posts/cuda-matmul-by-hand/><link crossorigin=anonymous href=/assets/css/stylesheet.8fe10233a706bc87f2e08b3cf97b8bd4c0a80f10675a143675d59212121037c0.css integrity="sha256-j+ECM6cGvIfy4Is8+XuL1MCoDxBnWhQ2ddWSEhIQN8A=" rel="preload stylesheet" as=style><link rel=icon href=https://blog.suhith.com/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://blog.suhith.com/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://blog.suhith.com/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://blog.suhith.com/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://blog.suhith.com/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://blog.suhith.com/posts/cuda-matmul-by-hand/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link href=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.css rel=stylesheet><meta property="og:url" content="https://blog.suhith.com/posts/cuda-matmul-by-hand/"><meta property="og:site_name" content="Suhith"><meta property="og:title" content="How a tiled matmul works"><meta property="og:description" content="Writing a tiled CUDA matmul üöÄ This year during my time at the Recurse Center, I worked through the various optimizations presented in Simon Boehm‚Äôs iconic post How to Optimize a CUDA Matmul Kernel for cuBLAS-like Performance: a Worklog. I approached these as a series of puzzles where I would read as little as possible, just the title or the first paragraphs describing the algorithm and then implement it in CUDA/C++. This was an exercise in writing & debugging CUDA along with implementing kernel code from a high level algorithm."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-09-15T17:01:20-07:00"><meta property="article:modified_time" content="2025-09-15T17:01:20-07:00"><meta property="og:image" content="https://blog.suhith.com/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://blog.suhith.com/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="How a tiled matmul works"><meta name=twitter:description content="Writing a tiled CUDA matmul üöÄ
This year during my time at the Recurse Center, I worked through the various optimizations presented in Simon Boehm&rsquo;s iconic post How to Optimize a CUDA Matmul Kernel for cuBLAS-like Performance: a Worklog. I approached these as a series of puzzles where I would read as little as possible, just the title or the first paragraphs describing the algorithm and then implement it in CUDA/C++. This was an exercise in writing & debugging CUDA along with implementing kernel code from a high level algorithm."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://blog.suhith.com/posts/"},{"@type":"ListItem","position":2,"name":"How a tiled matmul works","item":"https://blog.suhith.com/posts/cuda-matmul-by-hand/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"How a tiled matmul works","name":"How a tiled matmul works","description":"Writing a tiled CUDA matmul üöÄ This year during my time at the Recurse Center, I worked through the various optimizations presented in Simon Boehm\u0026rsquo;s iconic post How to Optimize a CUDA Matmul Kernel for cuBLAS-like Performance: a Worklog. I approached these as a series of puzzles where I would read as little as possible, just the title or the first paragraphs describing the algorithm and then implement it in CUDA/C++. This was an exercise in writing \u0026amp; debugging CUDA along with implementing kernel code from a high level algorithm.\n","keywords":[],"articleBody":"Writing a tiled CUDA matmul üöÄ This year during my time at the Recurse Center, I worked through the various optimizations presented in Simon Boehm‚Äôs iconic post How to Optimize a CUDA Matmul Kernel for cuBLAS-like Performance: a Worklog. I approached these as a series of puzzles where I would read as little as possible, just the title or the first paragraphs describing the algorithm and then implement it in CUDA/C++. This was an exercise in writing \u0026 debugging CUDA along with implementing kernel code from a high level algorithm.\nHere by matmul I actually mean a generalized matrix multiplication or gemm. Basically, a matrix multiply with some accoutrements thrown in. C‚ÜêŒ±AB+Œ≤C C \\leftarrow \\alpha AB + \\beta C C‚ÜêŒ±AB+Œ≤Chere‚Äôs our typical function signature\n__global__ void sgemm(int M, int N, int K, float alpha, const float *A, const float *B, float beta, float *C) The shapes are AAA as M√óKM \\times KM√óK, BBB as K√óNK \\times NK√óN and CCC as M√óNM \\times NM√óN.\nNaive Kernel Here we simply launch a thread for each output element in CCC and each thread independently computes the dot product of it‚Äôs respective row from AAA \u0026 column from BBB.\nOur launcher creates a grid of blocks, where each block is a group of 1024 threads referenced by a (32, 32) matrix. I won‚Äôt go into the details of this too much but section 5 of NVIDIA‚Äôs CUDA docs provides an explanation of the programming model.\ndim3 gridDim(CEIL_DIV(M, 32), CEIL_DIV(N, 32)); dim3 blockDim(32, 32); As you may expect, this approach is not very fast. There are many repeated reads from global memory (where all of A, B, and C live) and we don‚Äôt utilize the GPU‚Äôs memory bandwidth nor its strengths very well (more about this later).\n__global__ void sgemm_naive(int M, int N, int K, float alpha, const float *A, const float *B, float beta, float *C) { // this is 32 const uint x = blockIdx.x * blockDim.x + threadIdx.x; const uint y = blockIdx.y * blockDim.y + threadIdx.y; if (x \u003c M \u0026\u0026 y \u003c N) { float tmp = 0.0; for (int i = 0; i \u003c K; ++i) { tmp += A[x * K + i] * B[i * N + y]; } C[x * N + y] = alpha * tmp + beta * C[x * N + y]; } } This kernel‚Äôs performance is ‚Üí\\rightarrow‚Üí 553.3 GFLOPS\nGlobal Memory Coalescing With the naive kernel, we are not making good use of global memory (HBM) bandwidth. Each read from global memory by a thread is separate. How can we tweak our mapping of threads ‚Üí\\rightarrow‚Üí memory such that consecutive threads in a warp access consecutive memory locations such that GPU will coalesce (combine) global memory accesses. The compiler doesn‚Äôt issue any specific SASS/PTX for this. But the performance boost is significant. Yes, it‚Äôs subtle.\n// block size is 32 const uint x = blockIdx.y * 32 + threadIdx.y; const uint y = blockIdx.x * 32 + threadIdx.x; This kernel‚Äôs performance is ‚Üí\\rightarrow‚Üí 4042.3 GFLOPS\nShared Memory Blocking If we inspect the above figure showing global memory coalescing, 2 adjacent threads (by threadId) will each issue instructions to read from the same memory locations of AAA. Why fetch the same elements from global GPU memory multiple times?\nWe have access to faster on-chip memory called Shared Memory (SMEM). Each Streaming Multiprocessor has Shared Memory and it‚Äôs divided up between blocks of threads, such that threads in the same block can access the same section of shared memory. The amount of shared memory per SM (Streaming Multiprocessor) varies depending on the compute type. On our RTX 4090 this is 100KB per SM.\nThe shared memory blocking technique involves using the shared memory as a scratch space for the thread block. We map a thread block to a section of indices in matrix CCC. Now note that this result section requires computing the respective dot products from the shared memory (which the entire threadblock worked together to load). Then we can slide the cached block to obtain the next section and repeat.\nMy annotations are in brown, base image from Kernel 3 in siboehm‚Äôs matmul post\nMap the cRow, cCol from the y, x locations of the thread within the launch grid. We still launch one thread for each output element in CCC. However within the confines of our threadblock we can directly map into our shared memory section with row, col ‚Üí\\rightarrow‚Üí threadIdx.y, threadIdx.x.\nThis requires some thinking about the matrices and indices of the elements that interest us. They have been denoted with the brown annotated text. This boils down to keeping track of the matrix shape within which we want to index into.\nconst uint cCol = blockIdx.x * blockDim.x + threadIdx.x; const uint cRow = blockIdx.y * blockDim.y + threadIdx.y; // The row and column within the shared memory chunk const uint shmem_col = threadIdx.x; const uint shmem_row = threadIdx.y; // this is because the shmem size == block size __shared__ float sA[BLOCK_SIDE * BLOCK_SIDE]; __shared__ float sB[BLOCK_SIDE * BLOCK_SIDE]; for (int offset = 0; offset \u003c K; offset += 32) { // This should be a coalesced load because the rightmost value shmem_col \u0026 cCol both // reduce to threadIdx.x sA[shmem_row * BLOCK_SIDE + shmem_col] = A[cRow * K + (offset + shmem_col)]; // the same row (x) of the result element (x, y) // with the column offset by (offset + tid.y) sB[shmem_row * BLOCK_SIDE + shmem_col] = B[(offset + shmem_row) * N + cCol]; // the same column of the result element (x, y) ... // all the threads in the block compute the resulting dot products This kernel‚Äôs performance is ‚Üí\\rightarrow‚Üí 7402 GFLOPS\n1D Shared Memory Blocking Now let‚Äôs make each thread do a bit more work. Right now each thread is responsible for computing just a single result in CCC. Let‚Äôs give them some more responsibility. It can now compute a number of consecutive elements of the result, a 1D array of elements so to speak.\nMy annotations are on the right, left side is from Kernel 4 siboehm‚Äôs matmul post\nHere I find it‚Äôs useful to visualize the problem as disparate steps, where we have a number of threads at our disposal. Each thread does not need to work solely on computing its result, i.e., the thread responsible for computing specific elements in CCC may not be directly responsible for loading the contributing elements of AAA and BBB for its element.\nThe threads in a block we have to work with. Here we 512 threads compute 8 results each in a 64x64 section of CCC. (512‚àó8=64‚àó64=4096512 * 8 = 64 * 64 = 4096512‚àó8=64‚àó64=4096 results)\nWe have shared memory and __syncthreads() synchronization points, so we can share this work among different threads in a way that is convenient to us. For the elements local to a thread (the row of 8 elements a thread is responsible for in this example) we use registers, not shared memory. This is simply defined as float tmp[8] = {0.0};.\nMapping our threads to elements in AAA.\nLoading to shared memory from a 64x8 tile of AAA.\nconst uint a_inner_col = threadIdx.x % BK; const uint a_inner_row = threadIdx.x / BK; __shared__ float sA[BM * BK]; ... // moving the pointer to A to it's position in each threadblock is MUCH easier // than fiddling with indices trying to keep things consistent within the offset loop. A += blockIdx.y * BN * K; // blockIdx.y is the row of the CUDA threadblock in the launch grid. ... ... // within a loop incrementing our offset BK for (int offset = 0; offset \u003c K; offset += BK) { // the same row (x) of the result element (x, y) // with the column offset by (offset + tid.y) sA[a_inner_row * BK + a_inner_col] = A[a_inner_row * K + a_inner_col]; Mapping our threads to elements in BBB.\nLoading to shared memory from an 8x64 tile of BBB.\nconst uint b_inner_col = threadIdx.x % BN; // 0..63 const uint b_inner_row = threadIdx.x / BN; // 0..8 __shared__ float sB[BK * BN]; ... B += blockIdx.x * BM; ... ... // within a loop incrementing our offset BK for (int offset = 0; offset \u003c K; offset += BK) { // the same column of the result element (x, y) // with the row offset by (offset + tid.x) * N sB[b_inner_row * BN + b_inner_col] = B[(b_inner_row)*N + b_inner_col]; Mapping our threads to elements in CCC.\nWe map a single thread to 8 elements in CCC. This is where the increased work for each thread happens.\nconst uint c_row = threadIdx.x / BM; // 0..8 since each thread handles 8 elements const uint c_col = threadIdx.x % BM; ... ... // within a loop incrementing our offset BK for (int offset = 0; offset \u003c K; offset += BK) { ... ... for (int t = 0; t \u003c 8; t++) { for (int idx = 0; idx \u003c BK; idx++) { tmp[t] += sA[((c_row * 8) + t) * BK + idx ] * sB[idx * BN + c_col]; } } __syncthreads(); } for (int t = 0; t \u003c 8; t++) { C[(c_row * 8 + t) * N + c_col] = alpha * tmp[t] + beta * C[(c_row * 8 + t) * N + c_col]; } This kernel‚Äôs performance is ‚Üí\\rightarrow‚Üí 21808 GFLOPS\n2D Shared Memory Blocking Now that we have each thread computing consecutive values in a row, let‚Äôs increase the parallelism and have each thread compute values in a 2D grid to give even more work to the threads.\nOuter loop (similar to the 1D case)\nInner loop, each of the 3 squares in C is the territory of a single thread (drawn as 4x4 for simplicity)\nTo start let‚Äôs reason about how we would handle the loading and computation if we had specific tile sizes BM=64, BN=64, BK=8 and number of elements a thread computes TM=8, TN=8 thus 8 * 8 = 64 elements. This BM * BN section of C will be computed by a thread block.\nJust as before\nWhile loading A we could have had each of our 64 threads be dedicated to a row and load one column at a time. But this will lead to poor global memory coalescing. Instead the threads are divided into groups of 8, each loads an element of a row. Threads in a warp are assigned to memory locations in sequence to enable global memory coalescing.\nFor 64 threads (launched in a 1D block) these are mapped to elements of A like so:\nconst uint load_a_rows = threadIdx.x / BK // BK = 8 const uint load_a_cols = threadIdx.x % BK const uint strideA = numThreadsBlocktile / BK; // The total num of threads must be evenly divisible by BK // so we can skip strideA complete rows while loading a tile. assert(((numThreadsBlocktile) % BK == 0)); This in a loop that runs 8 times, and where the offset is updated by 64 each time will cover all the elements of our slice of shared memory.\nWe can map the 64 threads to elements of B like so. Each thread is assigned to a column element within the same row.\nconst uint load_b_cols = threadIdx % BN // BN = 64 const uint strideB = numThreadsBlocktile / BN; assert((numThreadsBlocktile) % BN == 0); We increment the row 8 times. This should become a coalesced read from memory.\nThe interesting sections of the code are below. After loading we accumulate into the tmp register.\n// The threads position within a grid of (BM/TM) x (BN/TN) const uint thread_col = threadIdx.x % (BN / TN); const uint thread_row = threadIdx.x / (BN / TN); A += blockIdx.y * BM * K; B += blockIdx.x * BN; C += (blockIdx.y * BM * N) + (blockIdx.x * BN); for (int offset = 0; offset \u003c K; offset += BK) { for (int shiftA = 0; shiftA \u003c BM; shiftA += strideA) { sA[((load_A_row + shiftA) * BK) + load_A_col] = A[((load_A_row + shiftA) * K) + load_A_col]; } for (int shiftB = 0; shiftB \u003c BK; shiftB += strideB) { sB[(load_B_row + shiftB) * BN + load_B_col] = B[(load_B_row + shiftB) * N + load_B_col]; } __syncthreads(); A += BK; B += BK * N; /* Note the use of a register caches (sharedA_cache, sharedB_cache) to reduce shared memory accesses. For example sB is hit just 1x per TN, not for every TM. */ for (int idx = 0; idx \u003c BK; ++idx) { for (int r = 0; r \u003c TM; ++r) { sharedA_cache[r] = sA[(thread_row * TM + r) * BK + idx]; } for (int c = 0; c \u003c TN; ++c) { sharedB_cache[c] = sB[(idx * BN) + (thread_col * TN + c)]; } for (int r = 0; r \u003c TM; ++r) { for (int c = 0; c \u003c TN; ++c) { tmp[r * TN + c] += sharedA_cache[r] * sharedB_cache[c]; } } } __syncthreads(); } This kernel‚Äôs performance is ‚Üí\\rightarrow‚Üí 37000 GFLOPS\nVectorized 2D Shared Memory Blocking The next optimization involves vectorization. Which loads can we vectorize? Loading from B into shared memory should be easy to vectorize as we‚Äôre already reading consecutive elements with consecutive threads. However with A our setup is a bit more complicated.\nThe ‚Äútrick‚Äù here is we transpose A upon its load into shared memory.\nWe point our threads at A very similarly to before (with the exception of each thread now loading a vector of 4 floats), but we send the elements to transposed locations in our shared memory locations for A known as sA.\nThe main differences in the kernel are with the loading from A.\nfor (int shiftA = 0; shiftA \u003c BM; shiftA += strideArows) { float4 tmp = reinterpret_cast\u003cfloat4*\u003e(\u0026A[((load_A_row + shiftA) * K) + load_A_col * 4])[0]; sA[(load_A_col * 4 + 0) * BM + load_A_row] = tmp.x; sA[(load_A_col * 4 + 1) * BM + load_A_row] = tmp.y; sA[(load_A_col * 4 + 2) * BM + load_A_row] = tmp.z; sA[(load_A_col * 4 + 3) * BM + load_A_row] = tmp.w; } We don‚Äôt make explicit changes to the loading from shared memory when we are computing products, as the compiler will issue lds.128 vectorized loads from shared memory in SASS.\nThis kernel‚Äôs performance is ‚Üí\\rightarrow‚Üí 39500 GFLOPS\nWarptiling Warptiling is our final optimization. While we use many of the patterns employed in the kernels we‚Äôve built up until now, the kernel is not much more complicated to write. To be honest, I spent the most time reading into why warptiling improves performance. The reasons are subtle and this is a good foundational kernel from which we can dip into using the fast Tensor Cores. When we add a level of hierarchy that aligns with the division of threads into warps, we get some performance benefits. In CUDA threads are executed together in groups of consecutive called warps (usually 32). A threadblock of 128 threads would consist of 4 warps. The benefits are to register access patterns and allows for some instruction level parallelism.\nBack to the gist of this post: how do we implement it? The main difference is that instead of spreading the threads out in a block, we tile the warps across the block. Thus the location of where, say threadIdx = 10 would be different as to where it was in the 2D tiling where the threads were just distributed all across the block.\nNote the threads (dots). The purple are warp 1 (threadIdx 0 to 31). The pink are warp 2.\nOnce we have filled our shared memory blocks, we load into registers at the warp level. We load sections of shared memory at a time into thread-local registers we call fragments. These load instructions will involve overlap between the threads and what they access from shared memory, and should execute as SIMT.\nThese are some of the new variables we define in our kernel to perform the matmul.\n/* TM * TN are the number of threads in the thread block thread_tile_rows * thread_tile_cols = 32 is our selected layout within the warp */ const uint warp_tile_cols = BN / (TN * thread_tile_cols); const uint warp_tile_rows = BM / (TM * thread_tile_rows); // shape of the warp tile assert(BN % (TN * thread_tile_cols) == 0); assert(BM % (TM * thread_tile_rows) == 0); // Warp location among the warp tiles const uint warp_row = warp_id / warp_tile_cols; const uint warp_col = warp_id % warp_tile_cols; // thread's x, y relative to warp const uint thread_tile_row = thread_lane / thread_tile_cols; const uint thread_tile_col = thread_lane % thread_tile_cols; This article in the CUTLASS documentation is a good resource to read more about warptiling.\nThis kernel‚Äôs performance is ‚Üí\\rightarrow‚Üí 42000 GFLOPS\nReflections I feel like this was a much slower method of learning to write CUDA matrix multiplications, abounding with frustrations and debugging. However, there was valuable insight in playing with these algorithms, drawing them (thanks excalidraw.com), and good practice in converting theoretical concepts to functional parallel code.\nNext are faster, more modern matrix multiplications. On certain shapes and leveraging tensor cores it should be possible to eke out a win on specific cuBLAS implementations.\n","wordCount":"2877","inLanguage":"en","image":"https://blog.suhith.com/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2025-09-15T17:01:20-07:00","dateModified":"2025-09-15T17:01:20-07:00","author":{"@type":"Person","name":"Suhith Rajesh"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.suhith.com/posts/cuda-matmul-by-hand/"},"publisher":{"@type":"Organization","name":"Suhith","logo":{"@type":"ImageObject","url":"https://blog.suhith.com/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://blog.suhith.com/ accesskey=h title="Suhith's Blog (Alt + H)"><img src=https://blog.suhith.com/apple-touch-icon.png alt aria-label=logo height=35>Suhith's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://blog.suhith.com/posts title=Posts><span>Posts</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://blog.suhith.com/>Home</a>&nbsp;¬ª&nbsp;<a href=https://blog.suhith.com/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">How a tiled matmul works</h1><div class=post-meta><span title='2025-09-15 17:01:20 -0700 PDT'>September 15, 2025</span>&nbsp;¬∑&nbsp;14 min&nbsp;¬∑&nbsp;Suhith Rajesh</div></header><div class=post-content><h1 id=writing-a-tiled-cuda-matmul->Writing a tiled CUDA matmul üöÄ<a hidden class=anchor aria-hidden=true href=#writing-a-tiled-cuda-matmul->#</a></h1><p>This year during my time at the <a href=https://www.recurse.com/>Recurse Center</a>, I worked through the various optimizations presented in Simon Boehm&rsquo;s iconic post <a href=https://siboehm.com/articles/22/CUDA-MMM>How to Optimize a CUDA Matmul Kernel for cuBLAS-like Performance: a Worklog</a>. I approached these as a series of puzzles where I would read as little as possible, just the title or the first paragraphs describing the algorithm and then implement it in CUDA/C++. This was an exercise in writing & debugging CUDA along with implementing kernel code from a high level algorithm.</p><p>Here by matmul I actually mean a <a href=https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms#Level_3><em>generalized matrix multiplication</em></a> or <strong>gemm</strong>. Basically, a matrix multiply with some accoutrements thrown in.</p><span class=katex-display><span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>C</mi><mo>‚Üê</mo><mi>Œ±</mi><mi>A</mi><mi>B</mi><mo>+</mo><mi>Œ≤</mi><mi>C</mi></mrow><annotation encoding="application/x-tex"> C \leftarrow \alpha AB + \beta C </annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.6833em></span><span class="mord mathnormal" style=margin-right:.07153em>C</span><span class=mspace style=margin-right:.2778em></span><span class=mrel>‚Üê</span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:.7667em;vertical-align:-.0833em></span><span class="mord mathnormal" style=margin-right:.0037em>Œ±</span><span class="mord mathnormal">A</span><span class="mord mathnormal" style=margin-right:.05017em>B</span><span class=mspace style=margin-right:.2222em></span><span class=mbin>+</span><span class=mspace style=margin-right:.2222em></span></span><span class=base><span class=strut style=height:.8889em;vertical-align:-.1944em></span><span class="mord mathnormal" style=margin-right:.07153em>Œ≤C</span></span></span></span></span><p>here&rsquo;s our typical function signature</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=n>__global__</span> <span class=kt>void</span> <span class=n>sgemm</span><span class=p>(</span><span class=kt>int</span> <span class=n>M</span><span class=p>,</span> <span class=kt>int</span> <span class=n>N</span><span class=p>,</span> <span class=kt>int</span> <span class=n>K</span><span class=p>,</span> <span class=kt>float</span> <span class=n>alpha</span><span class=p>,</span> <span class=k>const</span> <span class=kt>float</span> <span class=o>*</span><span class=n>A</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                            <span class=k>const</span> <span class=kt>float</span> <span class=o>*</span><span class=n>B</span><span class=p>,</span> <span class=kt>float</span> <span class=n>beta</span><span class=p>,</span> <span class=kt>float</span> <span class=o>*</span><span class=n>C</span><span class=p>)</span> 
</span></span></code></pre></div><p>The shapes are <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.6833em></span><span class="mord mathnormal">A</span></span></span></span> as <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mo>√ó</mo><mi>K</mi></mrow><annotation encoding="application/x-tex">M \times K</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.7667em;vertical-align:-.0833em></span><span class="mord mathnormal" style=margin-right:.10903em>M</span><span class=mspace style=margin-right:.2222em></span><span class=mbin>√ó</span><span class=mspace style=margin-right:.2222em></span></span><span class=base><span class=strut style=height:.6833em></span><span class="mord mathnormal" style=margin-right:.07153em>K</span></span></span></span>, <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.6833em></span><span class="mord mathnormal" style=margin-right:.05017em>B</span></span></span></span> as <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo>√ó</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">K \times N</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.7667em;vertical-align:-.0833em></span><span class="mord mathnormal" style=margin-right:.07153em>K</span><span class=mspace style=margin-right:.2222em></span><span class=mbin>√ó</span><span class=mspace style=margin-right:.2222em></span></span><span class=base><span class=strut style=height:.6833em></span><span class="mord mathnormal" style=margin-right:.10903em>N</span></span></span></span> and <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.6833em></span><span class="mord mathnormal" style=margin-right:.07153em>C</span></span></span></span> as <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mo>√ó</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">M \times N</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.7667em;vertical-align:-.0833em></span><span class="mord mathnormal" style=margin-right:.10903em>M</span><span class=mspace style=margin-right:.2222em></span><span class=mbin>√ó</span><span class=mspace style=margin-right:.2222em></span></span><span class=base><span class=strut style=height:.6833em></span><span class="mord mathnormal" style=margin-right:.10903em>N</span></span></span></span>.</p><h2 id=naive-kernel>Naive Kernel<a hidden class=anchor aria-hidden=true href=#naive-kernel>#</a></h2><p>Here we simply launch a thread for each output element in <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.6833em></span><span class="mord mathnormal" style=margin-right:.07153em>C</span></span></span></span> and each thread independently computes the dot product of it&rsquo;s respective row from <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.6833em></span><span class="mord mathnormal">A</span></span></span></span> & column from <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.6833em></span><span class="mord mathnormal" style=margin-right:.05017em>B</span></span></span></span>.</p><p>Our launcher creates a grid of blocks, where each block is a group of 1024 threads referenced by a (32, 32) matrix. I won&rsquo;t go into the details of this too much but <a href=https://docs.nvidia.com/cuda/cuda-c-programming-guide/#programming-model>section 5 of NVIDIA&rsquo;s CUDA docs</a> provides an explanation of the programming model.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl>  <span class=n>dim3</span> <span class=nf>gridDim</span><span class=p>(</span><span class=n>CEIL_DIV</span><span class=p>(</span><span class=n>M</span><span class=p>,</span> <span class=mi>32</span><span class=p>),</span> <span class=n>CEIL_DIV</span><span class=p>(</span><span class=n>N</span><span class=p>,</span> <span class=mi>32</span><span class=p>));</span>
</span></span><span class=line><span class=cl>  <span class=n>dim3</span> <span class=nf>blockDim</span><span class=p>(</span><span class=mi>32</span><span class=p>,</span> <span class=mi>32</span><span class=p>);</span>
</span></span></code></pre></div><p>As you may expect, this approach is not very fast. There are many repeated reads from global memory (where all of A, B, and C live) and we don&rsquo;t utilize the GPU&rsquo;s memory bandwidth nor its strengths very well (more about this later).</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=n>__global__</span> <span class=kt>void</span> <span class=nf>sgemm_naive</span><span class=p>(</span><span class=kt>int</span> <span class=n>M</span><span class=p>,</span> <span class=kt>int</span> <span class=n>N</span><span class=p>,</span> <span class=kt>int</span> <span class=n>K</span><span class=p>,</span> <span class=kt>float</span> <span class=n>alpha</span><span class=p>,</span> <span class=k>const</span> <span class=kt>float</span> <span class=o>*</span><span class=n>A</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                            <span class=k>const</span> <span class=kt>float</span> <span class=o>*</span><span class=n>B</span><span class=p>,</span> <span class=kt>float</span> <span class=n>beta</span><span class=p>,</span> <span class=kt>float</span> <span class=o>*</span><span class=n>C</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=c1>//                          this is 32
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=k>const</span> <span class=n>uint</span> <span class=n>x</span> <span class=o>=</span> <span class=n>blockIdx</span><span class=p>.</span><span class=n>x</span> <span class=o>*</span> <span class=n>blockDim</span><span class=p>.</span><span class=n>x</span> <span class=o>+</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=k>const</span> <span class=n>uint</span> <span class=n>y</span> <span class=o>=</span> <span class=n>blockIdx</span><span class=p>.</span><span class=n>y</span> <span class=o>*</span> <span class=n>blockDim</span><span class=p>.</span><span class=n>y</span> <span class=o>+</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>y</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>if</span> <span class=p>(</span><span class=n>x</span> <span class=o>&lt;</span> <span class=n>M</span> <span class=o>&amp;&amp;</span> <span class=n>y</span> <span class=o>&lt;</span> <span class=n>N</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>float</span> <span class=n>tmp</span> <span class=o>=</span> <span class=mf>0.0</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>K</span><span class=p>;</span> <span class=o>++</span><span class=n>i</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=n>tmp</span> <span class=o>+=</span> <span class=n>A</span><span class=p>[</span><span class=n>x</span> <span class=o>*</span> <span class=n>K</span> <span class=o>+</span> <span class=n>i</span><span class=p>]</span> <span class=o>*</span> <span class=n>B</span><span class=p>[</span><span class=n>i</span> <span class=o>*</span> <span class=n>N</span> <span class=o>+</span> <span class=n>y</span><span class=p>];</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=n>C</span><span class=p>[</span><span class=n>x</span> <span class=o>*</span> <span class=n>N</span> <span class=o>+</span> <span class=n>y</span><span class=p>]</span> <span class=o>=</span> <span class=n>alpha</span> <span class=o>*</span> <span class=n>tmp</span> <span class=o>+</span> <span class=n>beta</span> <span class=o>*</span> <span class=n>C</span><span class=p>[</span><span class=n>x</span> <span class=o>*</span> <span class=n>N</span> <span class=o>+</span> <span class=n>y</span><span class=p>];</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>This kernel&rsquo;s performance is <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>‚Üí</mo></mrow><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.3669em></span><span class=mrel>‚Üí</span></span></span></span> <strong>553.3 GFLOPS</strong></p><h2 id=global-memory-coalescing>Global Memory Coalescing<a hidden class=anchor aria-hidden=true href=#global-memory-coalescing>#</a></h2><p>With the naive kernel, we are not making good use of global memory (HBM) bandwidth. Each read from global memory by a thread is separate. <strong>How can we tweak our mapping of threads <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>‚Üí</mo></mrow><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.3669em></span><span class=mrel>‚Üí</span></span></span></span> memory such that consecutive threads in a warp access consecutive memory locations</strong> such that GPU will coalesce (combine) global memory accesses. The compiler doesn&rsquo;t issue any specific SASS/PTX for this. But the performance boost is significant. Yes, it&rsquo;s subtle.</p><p><img alt="global memory coalescing shown in excalidraw" loading=lazy src=/posts/cuda-matmul-by-hand/global_memory_clsc.png#center></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl>  <span class=c1>// block size is 32
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=k>const</span> <span class=n>uint</span> <span class=n>x</span> <span class=o>=</span> <span class=n>blockIdx</span><span class=p>.</span><span class=n>y</span> <span class=o>*</span> <span class=mi>32</span> <span class=o>+</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>y</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=k>const</span> <span class=n>uint</span> <span class=n>y</span> <span class=o>=</span> <span class=n>blockIdx</span><span class=p>.</span><span class=n>x</span> <span class=o>*</span> <span class=mi>32</span> <span class=o>+</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span><span class=p>;</span>
</span></span></code></pre></div><p>This kernel&rsquo;s performance is <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>‚Üí</mo></mrow><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.3669em></span><span class=mrel>‚Üí</span></span></span></span> <strong>4042.3 GFLOPS</strong></p><h2 id=shared-memory-blocking>Shared Memory Blocking<a hidden class=anchor aria-hidden=true href=#shared-memory-blocking>#</a></h2><p>If we inspect the above figure showing global memory coalescing, 2 adjacent threads (by <code>threadId</code>) will each issue instructions to read from the same memory locations of <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.6833em></span><span class="mord mathnormal">A</span></span></span></span>. Why fetch the same elements from global GPU memory multiple times?</p><p>We have access to faster on-chip memory called Shared Memory (SMEM). Each Streaming Multiprocessor has Shared Memory and it&rsquo;s divided up between blocks of threads, such that threads in the same block can access the same section of shared memory. The amount of shared memory per SM (Streaming Multiprocessor) varies depending on the compute type. On our RTX 4090 this is 100KB per SM.</p><p>The shared memory blocking technique involves using the shared memory as a scratch space for the thread block. We map a thread block to a section of indices in matrix <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.6833em></span><span class="mord mathnormal" style=margin-right:.07153em>C</span></span></span></span>. Now note that this result section requires computing the respective dot products from the shared memory (which the entire threadblock worked together to load). Then we can slide the cached block to obtain the next section and repeat.</p><figure class=align-center><img loading=lazy src=annotated_shared_memory_tiling.png#center alt="(annotated) shared memory tiling"><figcaption><p>My annotations are in brown, base image from Kernel 3 in <a href=https://siboehm.com/articles/22/CUDA-MMM>siboehm&rsquo;s matmul post</a></p></figcaption></figure><p>Map the <code>cRow, cCol</code> from the <code>y, x</code> locations of the thread within the launch grid. We still launch one thread for each output element in <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.6833em></span><span class="mord mathnormal" style=margin-right:.07153em>C</span></span></span></span>. However <strong>within the confines</strong> of our threadblock we can directly map into our shared memory section with <code>row, col</code> <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>‚Üí</mo></mrow><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.3669em></span><span class=mrel>‚Üí</span></span></span></span> <code>threadIdx.y, threadIdx.x</code>.</p><p>This requires some thinking about the matrices and indices of the elements that interest us. They have been denoted with the brown annotated text. This boils down to keeping track of the matrix shape within which we want to index into.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl>  <span class=k>const</span> <span class=n>uint</span> <span class=n>cCol</span> <span class=o>=</span> <span class=n>blockIdx</span><span class=p>.</span><span class=n>x</span> <span class=o>*</span> <span class=n>blockDim</span><span class=p>.</span><span class=n>x</span> <span class=o>+</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=k>const</span> <span class=n>uint</span> <span class=n>cRow</span> <span class=o>=</span> <span class=n>blockIdx</span><span class=p>.</span><span class=n>y</span> <span class=o>*</span> <span class=n>blockDim</span><span class=p>.</span><span class=n>y</span> <span class=o>+</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>y</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1>// The row and column within the shared memory chunk
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=k>const</span> <span class=n>uint</span> <span class=n>shmem_col</span> <span class=o>=</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=k>const</span> <span class=n>uint</span> <span class=n>shmem_row</span> <span class=o>=</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>y</span><span class=p>;</span> <span class=c1>// this is because the shmem size == block size
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl>  <span class=n>__shared__</span> <span class=kt>float</span> <span class=n>sA</span><span class=p>[</span><span class=n>BLOCK_SIDE</span> <span class=o>*</span> <span class=n>BLOCK_SIDE</span><span class=p>];</span>
</span></span><span class=line><span class=cl>  <span class=n>__shared__</span> <span class=kt>float</span> <span class=n>sB</span><span class=p>[</span><span class=n>BLOCK_SIDE</span> <span class=o>*</span> <span class=n>BLOCK_SIDE</span><span class=p>];</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>offset</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>offset</span> <span class=o>&lt;</span> <span class=n>K</span><span class=p>;</span> <span class=n>offset</span> <span class=o>+=</span> <span class=mi>32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=c1>// This should be a coalesced load because the rightmost value shmem_col &amp; cCol both
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=c1>// reduce to threadIdx.x
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=n>sA</span><span class=p>[</span><span class=n>shmem_row</span> <span class=o>*</span> <span class=n>BLOCK_SIDE</span> <span class=o>+</span> <span class=n>shmem_col</span><span class=p>]</span> <span class=o>=</span> <span class=n>A</span><span class=p>[</span><span class=n>cRow</span> <span class=o>*</span> <span class=n>K</span> <span class=o>+</span> <span class=p>(</span><span class=n>offset</span> <span class=o>+</span> <span class=n>shmem_col</span><span class=p>)];</span> <span class=c1>// the same row (x) of the result element (x, y)
</span></span></span><span class=line><span class=cl><span class=c1></span>                                                                                 <span class=c1>// with the column offset by (offset + tid.y)
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=n>sB</span><span class=p>[</span><span class=n>shmem_row</span> <span class=o>*</span> <span class=n>BLOCK_SIDE</span> <span class=o>+</span> <span class=n>shmem_col</span><span class=p>]</span> <span class=o>=</span> <span class=n>B</span><span class=p>[(</span><span class=n>offset</span> <span class=o>+</span> <span class=n>shmem_row</span><span class=p>)</span> <span class=o>*</span> <span class=n>N</span> <span class=o>+</span> <span class=n>cCol</span><span class=p>];</span> <span class=c1>// the same column of the result element (x, y)
</span></span></span><span class=line><span class=cl><span class=c1></span>                         
</span></span><span class=line><span class=cl>  <span class=p>...</span>
</span></span><span class=line><span class=cl>  <span class=c1>// all the threads in the block compute the resulting dot products
</span></span></span></code></pre></div><p>This kernel&rsquo;s performance is <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>‚Üí</mo></mrow><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.3669em></span><span class=mrel>‚Üí</span></span></span></span> <strong>7402 GFLOPS</strong></p><h2 id=1d-shared-memory-blocking>1D Shared Memory Blocking<a hidden class=anchor aria-hidden=true href=#1d-shared-memory-blocking>#</a></h2><p>Now let&rsquo;s make each thread do a bit more work. Right now each thread is responsible for computing just a single result in <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.6833em></span><span class="mord mathnormal" style=margin-right:.07153em>C</span></span></span></span>. Let&rsquo;s give them some more responsibility. It can now compute a number of consecutive elements of the result, a 1D array of elements so to speak.</p><figure class=align-center><img loading=lazy src=annotated_1d_shared_memory_tiling.png#center alt="(annotated) 1D shared memory tiling"><figcaption><p>My annotations are on the right, left side is from Kernel 4 <a href=https://siboehm.com/articles/22/CUDA-MMM>siboehm&rsquo;s matmul post</a></p></figcaption></figure><p>Here I find it&rsquo;s useful to visualize the problem as disparate steps, where we have a number of threads at our disposal. <strong>Each thread does not need to work solely on computing its result</strong>, i.e., the thread responsible for computing specific elements in <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.6833em></span><span class="mord mathnormal" style=margin-right:.07153em>C</span></span></span></span> may not be directly responsible for loading the contributing elements of <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.6833em></span><span class="mord mathnormal">A</span></span></span></span> and <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.6833em></span><span class="mord mathnormal" style=margin-right:.05017em>B</span></span></span></span> for its element.</p><figure class=align-center><img loading=lazy src=1d_threadpool.png#center alt="pool of 1d threads" height=200><figcaption><p>The threads in a block we have to work with. Here we 512 threads compute 8 results each in a 64x64 section of <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.6833em></span><span class="mord mathnormal" style=margin-right:.07153em>C</span></span></span></span>. (<span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>512</mn><mo>‚àó</mo><mn>8</mn><mo>=</mo><mn>64</mn><mo>‚àó</mo><mn>64</mn><mo>=</mo><mn>4096</mn></mrow><annotation encoding="application/x-tex">512 * 8 = 64 * 64 = 4096</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.6444em></span><span class=mord>512</span><span class=mspace style=margin-right:.2222em></span><span class=mbin>‚àó</span><span class=mspace style=margin-right:.2222em></span></span><span class=base><span class=strut style=height:.6444em></span><span class=mord>8</span><span class=mspace style=margin-right:.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:.6444em></span><span class=mord>64</span><span class=mspace style=margin-right:.2222em></span><span class=mbin>‚àó</span><span class=mspace style=margin-right:.2222em></span></span><span class=base><span class=strut style=height:.6444em></span><span class=mord>64</span><span class=mspace style=margin-right:.2778em></span><span class=mrel>=</span><span class=mspace style=margin-right:.2778em></span></span><span class=base><span class=strut style=height:.6444em></span><span class=mord>4096</span></span></span></span> results)</p></figcaption></figure><p>We have shared memory and <code>__syncthreads()</code> synchronization points, so we can share this work among different threads in a way that is convenient to us. For the elements local to a thread (the row of 8 elements a thread is responsible for in this example) we use registers, not shared memory. This is simply defined as <code>float tmp[8] = {0.0};</code>.</p><p><figure class=align-center><img loading=lazy src=1d_load_a_threads.png#center alt="Mapping our threads to elements in AAA." height=300><figcaption><p>Mapping our threads to elements in <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.6833em></span><span class="mord mathnormal">A</span></span></span></span>.</p></figcaption></figure>Loading to shared memory from a 64x8 tile of <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.6833em></span><span class="mord mathnormal">A</span></span></span></span>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl>  <span class=k>const</span> <span class=n>uint</span> <span class=n>a_inner_col</span> <span class=o>=</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span> <span class=o>%</span> <span class=n>BK</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=k>const</span> <span class=n>uint</span> <span class=n>a_inner_row</span> <span class=o>=</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span> <span class=o>/</span> <span class=n>BK</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=n>__shared__</span> <span class=kt>float</span> <span class=n>sA</span><span class=p>[</span><span class=n>BM</span> <span class=o>*</span> <span class=n>BK</span><span class=p>];</span>
</span></span><span class=line><span class=cl>  <span class=p>...</span>
</span></span><span class=line><span class=cl>  <span class=c1>// moving the pointer to A to it&#39;s position in each threadblock is MUCH easier
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=c1>// than fiddling with indices trying to keep things consistent within the offset loop.
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=n>A</span> <span class=o>+=</span> <span class=n>blockIdx</span><span class=p>.</span><span class=n>y</span> <span class=o>*</span> <span class=n>BN</span> <span class=o>*</span> <span class=n>K</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=c1>// blockIdx.y is the row of the CUDA threadblock in the launch grid.
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=p>...</span> 
</span></span><span class=line><span class=cl>  <span class=p>...</span>
</span></span><span class=line><span class=cl>  <span class=c1>// within a loop incrementing our offset BK
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>offset</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>offset</span> <span class=o>&lt;</span> <span class=n>K</span><span class=p>;</span> <span class=n>offset</span> <span class=o>+=</span> <span class=n>BK</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=c1>// the same row (x) of the result element (x, y)
</span></span></span><span class=line><span class=cl><span class=c1></span>      <span class=c1>// with the column offset by (offset + tid.y)
</span></span></span><span class=line><span class=cl><span class=c1></span>      <span class=n>sA</span><span class=p>[</span><span class=n>a_inner_row</span> <span class=o>*</span> <span class=n>BK</span> <span class=o>+</span> <span class=n>a_inner_col</span><span class=p>]</span> <span class=o>=</span> <span class=n>A</span><span class=p>[</span><span class=n>a_inner_row</span> <span class=o>*</span> <span class=n>K</span> <span class=o>+</span> <span class=n>a_inner_col</span><span class=p>];</span>
</span></span></code></pre></div><p><figure class=align-center><img loading=lazy src=1d_load_b_threads.png#center alt="Mapping our threads to elements in BBB." height=300><figcaption><p>Mapping our threads to elements in <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.6833em></span><span class="mord mathnormal" style=margin-right:.05017em>B</span></span></span></span>.</p></figcaption></figure>Loading to shared memory from an 8x64 tile of <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.6833em></span><span class="mord mathnormal" style=margin-right:.05017em>B</span></span></span></span>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl>  <span class=k>const</span> <span class=n>uint</span> <span class=n>b_inner_col</span> <span class=o>=</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span> <span class=o>%</span> <span class=n>BN</span><span class=p>;</span> <span class=c1>// 0..63
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=k>const</span> <span class=n>uint</span> <span class=n>b_inner_row</span> <span class=o>=</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span> <span class=o>/</span> <span class=n>BN</span><span class=p>;</span> <span class=c1>// 0..8
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=n>__shared__</span> <span class=kt>float</span> <span class=n>sB</span><span class=p>[</span><span class=n>BK</span> <span class=o>*</span> <span class=n>BN</span><span class=p>];</span>
</span></span><span class=line><span class=cl>  <span class=p>...</span>
</span></span><span class=line><span class=cl>  <span class=n>B</span> <span class=o>+=</span> <span class=n>blockIdx</span><span class=p>.</span><span class=n>x</span> <span class=o>*</span> <span class=n>BM</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=p>...</span>
</span></span><span class=line><span class=cl>  <span class=p>...</span>
</span></span><span class=line><span class=cl>  <span class=c1>// within a loop incrementing our offset BK
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>offset</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>offset</span> <span class=o>&lt;</span> <span class=n>K</span><span class=p>;</span> <span class=n>offset</span> <span class=o>+=</span> <span class=n>BK</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=c1>// the same column of the result element (x, y)
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=c1>// with the row offset by (offset + tid.x) * N
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=n>sB</span><span class=p>[</span><span class=n>b_inner_row</span> <span class=o>*</span> <span class=n>BN</span> <span class=o>+</span> <span class=n>b_inner_col</span><span class=p>]</span> <span class=o>=</span> <span class=n>B</span><span class=p>[(</span><span class=n>b_inner_row</span><span class=p>)</span><span class=o>*</span><span class=n>N</span> <span class=o>+</span> <span class=n>b_inner_col</span><span class=p>];</span>
</span></span></code></pre></div><p><figure class=align-center><img loading=lazy src=1d_load_c_threads.png#center alt="Mapping our threads to elements in CCC." height=300><figcaption><p>Mapping our threads to elements in <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.6833em></span><span class="mord mathnormal" style=margin-right:.07153em>C</span></span></span></span>.</p></figcaption></figure>We map a single thread to 8 elements in <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.6833em></span><span class="mord mathnormal" style=margin-right:.07153em>C</span></span></span></span>. This is where the increased work for each thread happens.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl>  <span class=k>const</span> <span class=n>uint</span> <span class=n>c_row</span> <span class=o>=</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span> <span class=o>/</span> <span class=n>BM</span><span class=p>;</span> <span class=c1>// 0..8 since each thread handles 8 elements
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=k>const</span> <span class=n>uint</span> <span class=n>c_col</span> <span class=o>=</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span> <span class=o>%</span> <span class=n>BM</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=p>...</span>
</span></span><span class=line><span class=cl>  <span class=p>...</span>
</span></span><span class=line><span class=cl>  <span class=c1>// within a loop incrementing our offset BK
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>offset</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>offset</span> <span class=o>&lt;</span> <span class=n>K</span><span class=p>;</span> <span class=n>offset</span> <span class=o>+=</span> <span class=n>BK</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=p>...</span>
</span></span><span class=line><span class=cl>    <span class=p>...</span>
</span></span><span class=line><span class=cl>      <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>t</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>t</span> <span class=o>&lt;</span> <span class=mi>8</span><span class=p>;</span> <span class=n>t</span><span class=o>++</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>idx</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>idx</span> <span class=o>&lt;</span> <span class=n>BK</span><span class=p>;</span> <span class=n>idx</span><span class=o>++</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=p>{</span>
</span></span><span class=line><span class=cl>          <span class=n>tmp</span><span class=p>[</span><span class=n>t</span><span class=p>]</span> <span class=o>+=</span> 
</span></span><span class=line><span class=cl>            <span class=n>sA</span><span class=p>[((</span><span class=n>c_row</span> <span class=o>*</span> <span class=mi>8</span><span class=p>)</span> <span class=o>+</span> <span class=n>t</span><span class=p>)</span> <span class=o>*</span> <span class=n>BK</span> <span class=o>+</span> <span class=n>idx</span>
</span></span><span class=line><span class=cl>            <span class=p>]</span> <span class=o>*</span> <span class=n>sB</span><span class=p>[</span><span class=n>idx</span> <span class=o>*</span> <span class=n>BN</span> <span class=o>+</span> <span class=n>c_col</span><span class=p>];</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>      <span class=p>}</span>
</span></span><span class=line><span class=cl>      <span class=n>__syncthreads</span><span class=p>();</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>t</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>t</span> <span class=o>&lt;</span> <span class=mi>8</span><span class=p>;</span> <span class=n>t</span><span class=o>++</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>C</span><span class=p>[(</span><span class=n>c_row</span> <span class=o>*</span> <span class=mi>8</span> <span class=o>+</span> <span class=n>t</span><span class=p>)</span> <span class=o>*</span> <span class=n>N</span> <span class=o>+</span> <span class=n>c_col</span><span class=p>]</span> <span class=o>=</span> <span class=n>alpha</span> <span class=o>*</span> <span class=n>tmp</span><span class=p>[</span><span class=n>t</span><span class=p>]</span> <span class=o>+</span> <span class=n>beta</span> <span class=o>*</span> <span class=n>C</span><span class=p>[(</span><span class=n>c_row</span> <span class=o>*</span> <span class=mi>8</span> <span class=o>+</span> <span class=n>t</span><span class=p>)</span> <span class=o>*</span> <span class=n>N</span> <span class=o>+</span> <span class=n>c_col</span><span class=p>];</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span></code></pre></div><p>This kernel&rsquo;s performance is <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>‚Üí</mo></mrow><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.3669em></span><span class=mrel>‚Üí</span></span></span></span> <strong>21808 GFLOPS</strong></p><h2 id=2d-shared-memory-blocking>2D Shared Memory Blocking<a hidden class=anchor aria-hidden=true href=#2d-shared-memory-blocking>#</a></h2><p>Now that we have each thread computing consecutive values in a row, let&rsquo;s increase the parallelism and have each thread compute values in a 2D grid to give even more work to the threads.</p><p><figure class=align-center><img loading=lazy src=2d_shared_memory_outer_loop.png#center alt="Outer loop (similar to the 1D case)" height=400><figcaption><p>Outer loop (similar to the 1D case)</p></figcaption></figure><figure class=align-center><img loading=lazy src=2d_shared_memory_tiling_inner_loop.png#center alt="Inner loop, each of the 3 squares in C is the territory of a single thread (drawn as 4x4 for simplicity)"><figcaption><p>Inner loop, each of the 3 squares in C is the territory of a single thread (drawn as 4x4 for simplicity)</p></figcaption></figure></p><p>To start let&rsquo;s reason about how we would handle the loading and computation if we had specific tile sizes <code>BM=64, BN=64, BK=8</code> and number of elements a thread computes <code>TM=8, TN=8</code> thus <code>8 * 8 = 64 elements</code>. This <code>BM * BN</code> section of <code>C</code> will be computed by a thread block.</p><figure class=align-center><img loading=lazy src=2d_threadpool.png#center alt="Just as before" height=300><figcaption><p>Just as before</p></figcaption></figure><p><figure class=align-center><img loading=lazy src=2d_load_a_threads.png#center height=400></figure>While loading A we could have had each of our 64 threads be dedicated to a row and load one column at a time. But this will lead to poor global memory coalescing. Instead the threads are divided into groups of 8, each loads an element of a row. Threads in a warp are assigned to memory locations in sequence to enable global memory coalescing.</p><p>For 64 threads (launched in a 1D block) these are mapped to elements of A like so:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=k>const</span> <span class=n>uint</span> <span class=n>load_a_rows</span> <span class=o>=</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span> <span class=o>/</span> <span class=n>BK</span> <span class=c1>// BK = 8
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>const</span> <span class=n>uint</span> <span class=n>load_a_cols</span> <span class=o>=</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span> <span class=o>%</span> <span class=n>BK</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>const</span> <span class=n>uint</span> <span class=n>strideA</span> <span class=o>=</span> <span class=n>numThreadsBlocktile</span> <span class=o>/</span> <span class=n>BK</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=c1>// The total num of threads must be evenly divisible by BK
</span></span></span><span class=line><span class=cl><span class=c1>// so we can skip strideA complete rows while loading a tile.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>assert</span><span class=p>(((</span><span class=n>numThreadsBlocktile</span><span class=p>)</span> <span class=o>%</span> <span class=n>BK</span> <span class=o>==</span> <span class=mi>0</span><span class=p>));</span> 
</span></span></code></pre></div><p>This in a loop that runs 8 times, and where the offset is updated by 64 each time will cover all the elements of our slice of shared memory.</p><p><figure class=align-center><img loading=lazy src=2d_load_b_threads.png#center height=300></figure>We can map the 64 threads to elements of B like so. Each thread is assigned to a column element within the same row.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=k>const</span> <span class=n>uint</span> <span class=n>load_b_cols</span> <span class=o>=</span> <span class=n>threadIdx</span> <span class=o>%</span> <span class=n>BN</span> <span class=c1>// BN = 64 
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl><span class=k>const</span> <span class=n>uint</span> <span class=n>strideB</span> <span class=o>=</span> <span class=n>numThreadsBlocktile</span> <span class=o>/</span> <span class=n>BN</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=n>assert</span><span class=p>((</span><span class=n>numThreadsBlocktile</span><span class=p>)</span> <span class=o>%</span> <span class=n>BN</span> <span class=o>==</span> <span class=mi>0</span><span class=p>);</span>
</span></span></code></pre></div><p>We increment the row 8 times. This should become a coalesced read from memory.</p><p>The interesting sections of the code are below. After loading we accumulate into the <code>tmp</code> register.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl>    <span class=c1>// The threads position within a grid of (BM/TM) x (BN/TN)
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>const</span> <span class=n>uint</span> <span class=n>thread_col</span> <span class=o>=</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span> <span class=o>%</span> <span class=p>(</span><span class=n>BN</span> <span class=o>/</span> <span class=n>TN</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=k>const</span> <span class=n>uint</span> <span class=n>thread_row</span> <span class=o>=</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span> <span class=o>/</span> <span class=p>(</span><span class=n>BN</span> <span class=o>/</span> <span class=n>TN</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>A</span> <span class=o>+=</span> <span class=n>blockIdx</span><span class=p>.</span><span class=n>y</span> <span class=o>*</span> <span class=n>BM</span> <span class=o>*</span> <span class=n>K</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>B</span> <span class=o>+=</span> <span class=n>blockIdx</span><span class=p>.</span><span class=n>x</span> <span class=o>*</span> <span class=n>BN</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>C</span> <span class=o>+=</span> <span class=p>(</span><span class=n>blockIdx</span><span class=p>.</span><span class=n>y</span> <span class=o>*</span> <span class=n>BM</span> <span class=o>*</span> <span class=n>N</span><span class=p>)</span> <span class=o>+</span> <span class=p>(</span><span class=n>blockIdx</span><span class=p>.</span><span class=n>x</span> <span class=o>*</span> <span class=n>BN</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>offset</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>offset</span> <span class=o>&lt;</span> <span class=n>K</span><span class=p>;</span> <span class=n>offset</span> <span class=o>+=</span> <span class=n>BK</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>shiftA</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>shiftA</span> <span class=o>&lt;</span> <span class=n>BM</span><span class=p>;</span> <span class=n>shiftA</span> <span class=o>+=</span> <span class=n>strideA</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=n>sA</span><span class=p>[((</span><span class=n>load_A_row</span> <span class=o>+</span> <span class=n>shiftA</span><span class=p>)</span> <span class=o>*</span> <span class=n>BK</span><span class=p>)</span> <span class=o>+</span> <span class=n>load_A_col</span><span class=p>]</span> <span class=o>=</span> <span class=n>A</span><span class=p>[((</span><span class=n>load_A_row</span> <span class=o>+</span> <span class=n>shiftA</span><span class=p>)</span> <span class=o>*</span> <span class=n>K</span><span class=p>)</span> <span class=o>+</span> <span class=n>load_A_col</span><span class=p>];</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>shiftB</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>shiftB</span> <span class=o>&lt;</span> <span class=n>BK</span><span class=p>;</span> <span class=n>shiftB</span> <span class=o>+=</span> <span class=n>strideB</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=n>sB</span><span class=p>[(</span><span class=n>load_B_row</span> <span class=o>+</span> <span class=n>shiftB</span><span class=p>)</span> <span class=o>*</span> <span class=n>BN</span> <span class=o>+</span> <span class=n>load_B_col</span><span class=p>]</span> <span class=o>=</span> <span class=n>B</span><span class=p>[(</span><span class=n>load_B_row</span> <span class=o>+</span> <span class=n>shiftB</span><span class=p>)</span> <span class=o>*</span> <span class=n>N</span> <span class=o>+</span> <span class=n>load_B_col</span><span class=p>];</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=n>__syncthreads</span><span class=p>();</span>
</span></span><span class=line><span class=cl>        <span class=n>A</span> <span class=o>+=</span> <span class=n>BK</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=n>B</span> <span class=o>+=</span> <span class=n>BK</span> <span class=o>*</span> <span class=n>N</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=cm>/*
</span></span></span><span class=line><span class=cl><span class=cm>        Note the use of a register caches (sharedA_cache, sharedB_cache)
</span></span></span><span class=line><span class=cl><span class=cm>        to reduce shared memory accesses.
</span></span></span><span class=line><span class=cl><span class=cm>        For example sB is hit just 1x per TN, not for every TM.
</span></span></span><span class=line><span class=cl><span class=cm>        */</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>idx</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>idx</span> <span class=o>&lt;</span> <span class=n>BK</span><span class=p>;</span> <span class=o>++</span><span class=n>idx</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>r</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>r</span> <span class=o>&lt;</span> <span class=n>TM</span><span class=p>;</span> <span class=o>++</span><span class=n>r</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=p>{</span>
</span></span><span class=line><span class=cl>                <span class=n>sharedA_cache</span><span class=p>[</span><span class=n>r</span><span class=p>]</span> <span class=o>=</span> <span class=n>sA</span><span class=p>[(</span><span class=n>thread_row</span> <span class=o>*</span> <span class=n>TM</span> <span class=o>+</span> <span class=n>r</span><span class=p>)</span> <span class=o>*</span> <span class=n>BK</span> <span class=o>+</span> <span class=n>idx</span><span class=p>];</span>
</span></span><span class=line><span class=cl>            <span class=p>}</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>c</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>c</span> <span class=o>&lt;</span> <span class=n>TN</span><span class=p>;</span> <span class=o>++</span><span class=n>c</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=p>{</span>
</span></span><span class=line><span class=cl>                <span class=n>sharedB_cache</span><span class=p>[</span><span class=n>c</span><span class=p>]</span> <span class=o>=</span> <span class=n>sB</span><span class=p>[(</span><span class=n>idx</span> <span class=o>*</span> <span class=n>BN</span><span class=p>)</span> <span class=o>+</span> <span class=p>(</span><span class=n>thread_col</span> <span class=o>*</span> <span class=n>TN</span> <span class=o>+</span> <span class=n>c</span><span class=p>)];</span>
</span></span><span class=line><span class=cl>            <span class=p>}</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>r</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>r</span> <span class=o>&lt;</span> <span class=n>TM</span><span class=p>;</span> <span class=o>++</span><span class=n>r</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=p>{</span>
</span></span><span class=line><span class=cl>                <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>c</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>c</span> <span class=o>&lt;</span> <span class=n>TN</span><span class=p>;</span> <span class=o>++</span><span class=n>c</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=p>{</span>
</span></span><span class=line><span class=cl>                    <span class=n>tmp</span><span class=p>[</span><span class=n>r</span> <span class=o>*</span> <span class=n>TN</span> <span class=o>+</span> <span class=n>c</span><span class=p>]</span> <span class=o>+=</span>
</span></span><span class=line><span class=cl>                        <span class=n>sharedA_cache</span><span class=p>[</span><span class=n>r</span><span class=p>]</span> <span class=o>*</span> <span class=n>sharedB_cache</span><span class=p>[</span><span class=n>c</span><span class=p>];</span>
</span></span><span class=line><span class=cl>                <span class=p>}</span>
</span></span><span class=line><span class=cl>            <span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=n>__syncthreads</span><span class=p>();</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span></code></pre></div><p>This kernel&rsquo;s performance is <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>‚Üí</mo></mrow><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.3669em></span><span class=mrel>‚Üí</span></span></span></span> <strong>37000 GFLOPS</strong></p><h2 id=vectorized-2d-shared-memory-blocking>Vectorized 2D Shared Memory Blocking<a hidden class=anchor aria-hidden=true href=#vectorized-2d-shared-memory-blocking>#</a></h2><p>The next optimization involves vectorization. Which loads can we vectorize? Loading from <code>B</code> into shared memory should be easy to vectorize as we&rsquo;re already reading consecutive elements with consecutive threads. However with <code>A</code> our setup is a bit more complicated.</p><p>The &ldquo;trick&rdquo; here is we transpose <code>A</code> upon its load into shared memory.</p><figure class=align-center><img loading=lazy src=vectorized_2d.png#center height=500></figure><p>We point our threads at <code>A</code> very similarly to before (with the exception of each thread now loading a vector of 4 floats), but we send the elements to transposed locations in our shared memory locations for <code>A</code> known as <code>sA</code>.</p><figure class=align-center><img loading=lazy src=vectorized_2d_load_a.png#center height=500></figure><p>The main differences in the kernel are with the loading from <code>A</code>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>shiftA</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>shiftA</span> <span class=o>&lt;</span> <span class=n>BM</span><span class=p>;</span> <span class=n>shiftA</span> <span class=o>+=</span> <span class=n>strideArows</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=n>float4</span> <span class=n>tmp</span> <span class=o>=</span> <span class=k>reinterpret_cast</span><span class=o>&lt;</span><span class=n>float4</span><span class=o>*&gt;</span><span class=p>(</span><span class=o>&amp;</span><span class=n>A</span><span class=p>[((</span><span class=n>load_A_row</span> <span class=o>+</span> <span class=n>shiftA</span><span class=p>)</span> <span class=o>*</span> <span class=n>K</span><span class=p>)</span> <span class=o>+</span> <span class=n>load_A_col</span> <span class=o>*</span> <span class=mi>4</span><span class=p>])[</span><span class=mi>0</span><span class=p>];</span>
</span></span><span class=line><span class=cl>        <span class=n>sA</span><span class=p>[(</span><span class=n>load_A_col</span> <span class=o>*</span> <span class=mi>4</span> <span class=o>+</span> <span class=mi>0</span><span class=p>)</span> <span class=o>*</span> <span class=n>BM</span> <span class=o>+</span> <span class=n>load_A_row</span><span class=p>]</span> <span class=o>=</span> <span class=n>tmp</span><span class=p>.</span><span class=n>x</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=n>sA</span><span class=p>[(</span><span class=n>load_A_col</span> <span class=o>*</span> <span class=mi>4</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span> <span class=o>*</span> <span class=n>BM</span> <span class=o>+</span> <span class=n>load_A_row</span><span class=p>]</span> <span class=o>=</span> <span class=n>tmp</span><span class=p>.</span><span class=n>y</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=n>sA</span><span class=p>[(</span><span class=n>load_A_col</span> <span class=o>*</span> <span class=mi>4</span> <span class=o>+</span> <span class=mi>2</span><span class=p>)</span> <span class=o>*</span> <span class=n>BM</span> <span class=o>+</span> <span class=n>load_A_row</span><span class=p>]</span> <span class=o>=</span> <span class=n>tmp</span><span class=p>.</span><span class=n>z</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=n>sA</span><span class=p>[(</span><span class=n>load_A_col</span> <span class=o>*</span> <span class=mi>4</span> <span class=o>+</span> <span class=mi>3</span><span class=p>)</span> <span class=o>*</span> <span class=n>BM</span> <span class=o>+</span> <span class=n>load_A_row</span><span class=p>]</span> <span class=o>=</span> <span class=n>tmp</span><span class=p>.</span><span class=n>w</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span></code></pre></div><p>We don&rsquo;t make explicit changes to the loading from shared memory when we are computing products, as the compiler will issue <code>lds.128</code> vectorized loads from shared memory in SASS.</p><p>This kernel&rsquo;s performance is <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>‚Üí</mo></mrow><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.3669em></span><span class=mrel>‚Üí</span></span></span></span> <strong>39500 GFLOPS</strong></p><h2 id=warptiling>Warptiling<a hidden class=anchor aria-hidden=true href=#warptiling>#</a></h2><p>Warptiling is our final optimization. While we use many of the patterns employed in the kernels we&rsquo;ve built up until now, the kernel is not much more complicated to write. To be honest, I spent the most time reading into <strong>why</strong> warptiling improves performance. The reasons are subtle and this is a good foundational kernel from which we can dip into using the fast Tensor Cores. When we add a level of hierarchy that aligns with the division of threads into warps, we get some performance benefits. In CUDA threads are executed together in groups of consecutive called <em>warps</em> (usually 32). A threadblock of 128 threads would consist of 4 warps.
The benefits are to register access patterns and allows for some instruction level parallelism.</p><p>Back to the gist of this post: how do we implement it? The main difference is that instead of spreading the threads out in a block, we tile the warps across the block. Thus the location of where, say <code>threadIdx = 10</code> would be different as to where it was in the 2D tiling where the threads were just distributed all across the block.</p><figure class=align-center><img loading=lazy src=warptiling_blocks.png#center alt="Note the threads (dots). The purple are warp 1 (threadIdx 0 to 31). The pink are warp 2." height=500><figcaption><p>Note the threads (dots). The purple are warp 1 (threadIdx 0 to 31). The pink are warp 2.</p></figcaption></figure><p>Once we have filled our shared memory blocks, we load into registers at the warp level. We load sections of shared memory at a time into thread-local registers we call fragments. These load instructions will involve overlap between the threads and what they access from shared memory, and should execute as SIMT.</p><figure class=align-center><img loading=lazy src=warptiling_fragments.png#center height=500></figure><p>These are some of the new variables we define in our kernel to perform the matmul.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl>  <span class=cm>/* 
</span></span></span><span class=line><span class=cl><span class=cm>  TM * TN are the number of threads in the thread block
</span></span></span><span class=line><span class=cl><span class=cm>  thread_tile_rows * thread_tile_cols = 32 is our selected layout within the warp
</span></span></span><span class=line><span class=cl><span class=cm>  */</span>
</span></span><span class=line><span class=cl>  <span class=k>const</span> <span class=n>uint</span> <span class=n>warp_tile_cols</span> <span class=o>=</span> <span class=n>BN</span> <span class=o>/</span> <span class=p>(</span><span class=n>TN</span> <span class=o>*</span> <span class=n>thread_tile_cols</span><span class=p>);</span> 
</span></span><span class=line><span class=cl>  <span class=k>const</span> <span class=n>uint</span> <span class=n>warp_tile_rows</span> <span class=o>=</span> <span class=n>BM</span> <span class=o>/</span> <span class=p>(</span><span class=n>TM</span> <span class=o>*</span> <span class=n>thread_tile_rows</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  <span class=c1>// shape of the warp tile
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=n>assert</span><span class=p>(</span><span class=n>BN</span> <span class=o>%</span> <span class=p>(</span><span class=n>TN</span> <span class=o>*</span> <span class=n>thread_tile_cols</span><span class=p>)</span> <span class=o>==</span> <span class=mi>0</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  <span class=n>assert</span><span class=p>(</span><span class=n>BM</span> <span class=o>%</span> <span class=p>(</span><span class=n>TM</span> <span class=o>*</span> <span class=n>thread_tile_rows</span><span class=p>)</span> <span class=o>==</span> <span class=mi>0</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1>// Warp location among the warp tiles
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=k>const</span> <span class=n>uint</span> <span class=n>warp_row</span> <span class=o>=</span> <span class=n>warp_id</span> <span class=o>/</span> <span class=n>warp_tile_cols</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=k>const</span> <span class=n>uint</span> <span class=n>warp_col</span> <span class=o>=</span> <span class=n>warp_id</span> <span class=o>%</span> <span class=n>warp_tile_cols</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1>// thread&#39;s x, y relative to warp
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=k>const</span> <span class=n>uint</span> <span class=n>thread_tile_row</span> <span class=o>=</span> <span class=n>thread_lane</span> <span class=o>/</span> <span class=n>thread_tile_cols</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=k>const</span> <span class=n>uint</span> <span class=n>thread_tile_col</span> <span class=o>=</span> <span class=n>thread_lane</span> <span class=o>%</span> <span class=n>thread_tile_cols</span><span class=p>;</span>
</span></span></code></pre></div><p><a href=https://github.com/NVIDIA/cutlass/blob/main/media/docs/cpp/efficient_gemm.md>This article</a> in the CUTLASS documentation is a good resource to read more about warptiling.</p><p>This kernel&rsquo;s performance is <span class=katex><span class=katex-mathml><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>‚Üí</mo></mrow><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=base><span class=strut style=height:.3669em></span><span class=mrel>‚Üí</span></span></span></span> <strong>42000 GFLOPS</strong></p><h3 id=reflections>Reflections<a hidden class=anchor aria-hidden=true href=#reflections>#</a></h3><p>I feel like this was a much slower method of learning to write CUDA matrix multiplications, abounding with frustrations and debugging. However, there was valuable insight in playing with these algorithms, drawing them (thanks <a href=https://excalidraw.com>excalidraw.com</a>), and good practice in converting theoretical concepts to functional parallel code.</p><p>Next are faster, more modern matrix multiplications. On certain shapes and leveraging tensor cores it should be possible to eke out a win on specific cuBLAS implementations.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://blog.suhith.com/>Suhith</a></span> ¬∑
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>